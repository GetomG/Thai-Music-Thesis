{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtTv6slDXh2i"
      },
      "source": [
        "# **V2** Thai Classical Music Generation – Baseline Sequence Modeling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfEWZfpKcEvs"
      },
      "source": [
        "\n",
        "## 0. Prerequisites & Setup\n",
        "\n",
        "- Reuse normalized symbolic dataset from Stage 2  \n",
        "- Pitch-only representation (octave stripped)  \n",
        "- Rest token included as explicit `<REST>` symbol  \n",
        "- Sequence = flattened token stream per song  \n",
        "\n",
        "**Goal:**  \n",
        "Train a baseline conditional LSTM language model on symbolic pitch sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEq7kOrFxuQx"
      },
      "source": [
        "### 0.0 Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rKCStsbyb8hK",
        "outputId": "c638c17e-a46a-4194-9f59-419fab6908b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mido in /usr/local/lib/python3.10/site-packages (1.3.0)\n",
            "Requirement already satisfied: python-rtmidi in /usr/local/lib/python3.10/site-packages (1.5.8)\n",
            "Requirement already satisfied: packaging~=23.1 in /Users/thanakrit/Library/Python/3.10/lib/python/site-packages (from mido) (23.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (4.66.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install mido python-rtmidi\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MOMxTrHTb9Sm",
        "outputId": "4c09de1a-1f7b-4ddb-ec67-1a881067d914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-02-21 15:54:54--  https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Phonbopit/sarabun-webfont/master/fonts/thsarabunnew-webfont.ttf [following]\n",
            "--2026-02-21 15:54:55--  https://raw.githubusercontent.com/Phonbopit/sarabun-webfont/master/fonts/thsarabunnew-webfont.ttf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 98308 (96K) [application/octet-stream]\n",
            "Saving to: ‘thsarabunnew-webfont.ttf.2’\n",
            "\n",
            "thsarabunnew-webfon 100%[===================>]  96.00K   388KB/s    in 0.2s    \n",
            "\n",
            "2026-02-21 15:54:57 (388 KB/s) - ‘thsarabunnew-webfont.ttf.2’ saved [98308/98308]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Colab-only: Clone repo & cd into it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XPvMIzPNNdjL",
        "outputId": "f0299675-e69c-4f39-d53b-214aae6ce733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Thai-Music-Thesis'...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 5] Input/output error",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/utils/_process_posix.py:153\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pexpect/spawnbase.py:372\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgit clone https://github.com/GetomG/Thai-Music-Thesis.git\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/ipykernel/zmqshell.py:649\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/utils/_process_posix.py:164\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    159\u001b[0m         out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mchr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pexpect/pty_spawn.py:578\u001b[0m, in \u001b[0;36mspawn.sendline\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Wraps send(), sending string ``s`` to child process, with\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m``os.linesep`` automatically appended. Returns number of bytes\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03mwritten.  Only a limited number of bytes may be sent for each\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03mline in the default terminal mode, see docstring of :meth:`send`.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    577\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coerce_send_string(s)\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinesep\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pexpect/pty_spawn.py:569\u001b[0m, in \u001b[0;36mspawn.send\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log(s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    568\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder\u001b[38;5;241m.\u001b[39mencode(s, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
          ]
        }
      ],
      "source": [
        "# Uncomment on first Colab run:\n",
        "# !git clone https://github.com/GetomG/Thai-Music-Thesis.git\n",
        "# %cd Thai-Music-Thesis\n",
        "\n",
        "# If already cloned:\n",
        "# !cd Thai-Music-Thesis && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ynk_wrhGNhAi"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Import Helper Utilities from thai_music_utils\n",
        "# ============================================================\n",
        "\n",
        "# 1. Notation Processing\n",
        "from thai_music_utils.notation_utils import (\n",
        "    flatten_song_notation,\n",
        "    normalize_octave_markers,\n",
        "    notation_to_sequence\n",
        ")\n",
        "\n",
        "# 2. Octave Inference (DP-based register guessing)\n",
        "from thai_music_utils.octave_inference import (\n",
        "    is_thai_note,\n",
        "    get_fixed_octave,\n",
        "    guess_octaves_with_constraints,\n",
        "    add_octaves_respecting_labels\n",
        ")\n",
        "\n",
        "# 3. Preprocessing Utilities\n",
        "from thai_music_utils.preprocessing import (\n",
        "    flatten_song_data,\n",
        "    remove_all_signs\n",
        ")\n",
        "\n",
        "# 4. EDA Helpers (Symbolic Analysis)\n",
        "from thai_music_utils.eda_symbolic_normalization import (\n",
        "    normalize_token,\n",
        "    normalize_bar,\n",
        "    flatten_song,\n",
        "    THAI_NOTES,\n",
        "    UP_MARK,\n",
        "    LOW_MARK,\n",
        "    REST_TOKEN\n",
        ")\n",
        "\n",
        "# 5. EDA Stats\n",
        "from thai_music_utils.eda_stats import (\n",
        "    extract_symbols,\n",
        "    pitch_stats,\n",
        "    stats_to_df\n",
        ")\n",
        "\n",
        "# 6. I/O Utilities\n",
        "from thai_music_utils.io_utils import (\n",
        "    save_json_bar_per_line\n",
        ")\n",
        "\n",
        "# 7. MIDI Rendering (Ranad-specific)\n",
        "from thai_music_utils.midi_ranad import (\n",
        "    generate_ranad_midi\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kzD-VM_ncaZZ"
      },
      "outputs": [],
      "source": [
        "#setting Thai fonts\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mpl.font_manager.fontManager.addfont('thsarabunnew-webfont.ttf')\n",
        "mpl.rc('font', family='TH Sarabun New')\n",
        "mpl.rcParams[\"axes.unicode_minus\"] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BMxPsIy6jubT"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9gBJXDzcfIx"
      },
      "source": [
        "### 0.1 Data Intake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpmkGQxAb_tC",
        "outputId": "13083f59-73fc-4551-92a2-566f58e41c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Runtime: Local\n",
            "DATA_ROOT: /Users/thanakrit/Documents/Thai Music Thesis/thai_music_data\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "from pathlib import Path\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    DATA_ROOT = Path(\"/content/drive/MyDrive/thai_music_data\")\n",
        "else:\n",
        "    # Local: data lives next to the notebook\n",
        "    DATA_ROOT = Path(os.path.abspath(\"\")).resolve() / \"thai_music_data\"\n",
        "\n",
        "print(f\"Runtime: {'Colab' if IS_COLAB else 'Local'}\")\n",
        "print(f\"DATA_ROOT: {DATA_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S33386eGcjjC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "BASE = DATA_ROOT / \"songs\"\n",
        "\n",
        "songs = []\n",
        "\n",
        "for motif_dir in BASE.iterdir():\n",
        "    if not motif_dir.is_dir():\n",
        "        continue\n",
        "\n",
        "    motif = motif_dir.name\n",
        "\n",
        "    for song_dir in motif_dir.iterdir():\n",
        "        json_dir = song_dir / \"json\"\n",
        "        if not json_dir.exists():\n",
        "            continue\n",
        "\n",
        "        for json_file in json_dir.glob(\"*.json\"):\n",
        "            try:\n",
        "                with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                songs.append({\n",
        "                    \"motif\": motif,\n",
        "                    \"song\": song_dir.name,\n",
        "                    \"path\": str(json_file),\n",
        "                    \"data\": data\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipped {json_file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4edkdRsDc-4S",
        "outputId": "c61ba744-d000-4e76-db5c-b4cd0f561f6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total songs loaded: 44\n",
            "เขมร: 7 songs\n",
            "จีน: 4 songs\n",
            "พม่า: 5 songs\n",
            "ลาว: 18 songs\n",
            "ไทยเดิม: 2 songs\n",
            "แขก: 8 songs\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total songs loaded: {len(songs)}\")\n",
        "\n",
        "by_motif = defaultdict(int)\n",
        "for s in songs:\n",
        "    by_motif[s[\"motif\"]] += 1\n",
        "\n",
        "for motif, n in by_motif.items():\n",
        "    print(f\"{motif}: {n} songs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5s7ltZVapgi3",
        "outputId": "fab813af-b252-4c6b-edcc-82e9c981c733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Note Count per Motif ===\n",
            "\n",
            "จีน:\n",
            "  Total notes (including rests): 1740\n",
            "  Songs: 4\n",
            "  Avg notes per song: 435.00\n",
            "\n",
            "พม่า:\n",
            "  Total notes (including rests): 971\n",
            "  Songs: 5\n",
            "  Avg notes per song: 194.20\n",
            "\n",
            "ลาว:\n",
            "  Total notes (including rests): 5706\n",
            "  Songs: 17\n",
            "  Avg notes per song: 335.65\n",
            "\n",
            "เขมร:\n",
            "  Total notes (including rests): 5936\n",
            "  Songs: 7\n",
            "  Avg notes per song: 848.00\n",
            "\n",
            "แขก:\n",
            "  Total notes (including rests): 5725\n",
            "  Songs: 8\n",
            "  Avg notes per song: 715.62\n",
            "\n",
            "ไทยเดิม:\n",
            "  Total notes (including rests): 836\n",
            "  Songs: 2\n",
            "  Avg notes per song: 418.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# from collections import defaultdict\n",
        "\n",
        "# note_count_by_motif = defaultdict(int)\n",
        "# song_count_by_motif = defaultdict(int)\n",
        "\n",
        "# for s in songs:\n",
        "#     motif = s[\"motif\"]\n",
        "#     note_count_by_motif[motif] += len(s[\"pitch_sequence\"])\n",
        "#     song_count_by_motif[motif] += 1\n",
        "\n",
        "# print(\"=== Note Count per Motif ===\\n\")\n",
        "\n",
        "# for motif in sorted(note_count_by_motif.keys()):\n",
        "#     total_notes = note_count_by_motif[motif]\n",
        "#     total_songs = song_count_by_motif[motif]\n",
        "#     avg_len = total_notes / total_songs\n",
        "\n",
        "#     print(f\"{motif}:\")\n",
        "#     print(f\"  Total notes (including rests): {total_notes}\")\n",
        "#     print(f\"  Songs: {total_songs}\")\n",
        "#     print(f\"  Avg notes per song: {avg_len:.2f}\")\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfUdMWpF144i"
      },
      "source": [
        "#### **Filter only Khmer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRW82DyQ14o7",
        "outputId": "7be11591-a3ed-45bb-c6c3-4568d2f19702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Khmer songs: 7\n"
          ]
        }
      ],
      "source": [
        "songs = [s for s in songs if s[\"motif\"] == \"เขมร\"]\n",
        "\n",
        "print(\"Total Khmer songs:\", len(songs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrCx28rvdZ35"
      },
      "source": [
        "### *0.2* Symbolic Normalization & Sequence Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5MXSmLdd5Za"
      },
      "source": [
        "#### 1️⃣ Normalize + Flatten\n",
        "\n",
        "Reuse the earlier symbolic normalization logic, but simplify it for sequence modeling.\n",
        "\n",
        "**Goal:**\n",
        "- Clean and standardize symbolic tokens\n",
        "- Decide whether to keep or remove register (pitch-only vs pitch+register)\n",
        "- Convert structured JSON (section → bar → token) into one linear sequence per song\n",
        "- Prepare clean token streams ready for vocabulary building and LSTM training\n",
        "\n",
        "Output:\n",
        "Each song → one flat symbolic sequence (list of tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqD-79VIbh5e"
      },
      "source": [
        "#### `normalize_token` & `song_to_pitch_sequence`\n",
        "\n",
        "These are the core functions for this notebook's tokenization:\n",
        "\n",
        "- `normalize_token` — Strips octave markers, splits dashes into compressed `<REST_k>` tokens  \n",
        "- `song_to_pitch_sequence` — Walks the full song JSON (sections → bars → tokens) and returns one flat token list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H8rMYa3Jd44k"
      },
      "outputs": [],
      "source": [
        "THAI_NOTES = set(\"ดรมฟซลท\")\n",
        "UP_MARK = \"ํ\"\n",
        "LOW_MARK = \"ฺ\"\n",
        "\n",
        "def normalize_token(token):\n",
        "    \"\"\"\n",
        "    Convert token into pitch tokens + compressed rest tokens.\n",
        "\n",
        "    Rest compression rule:\n",
        "    - Any number of consecutive dashes is decomposed into\n",
        "      chunks of <REST_4>, <REST_3>, <REST_2>, <REST_1>\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(token, str):\n",
        "        return [\"<REST_1>\"]\n",
        "\n",
        "    token = token.strip()\n",
        "\n",
        "    # ---- Pure rest token ----\n",
        "    if set(token) == {\"-\"}:\n",
        "        dash_count = len(token)\n",
        "        rests = []\n",
        "\n",
        "        while dash_count > 0:\n",
        "            if dash_count >= 4:\n",
        "                rests.append(\"<REST_4>\")\n",
        "                dash_count -= 4\n",
        "            elif dash_count >= 3:\n",
        "                rests.append(\"<REST_3>\")\n",
        "                dash_count -= 3\n",
        "            elif dash_count >= 2:\n",
        "                rests.append(\"<REST_2>\")\n",
        "                dash_count -= 2\n",
        "            else:\n",
        "                rests.append(\"<REST_1>\")\n",
        "                dash_count -= 1\n",
        "\n",
        "        return rests\n",
        "\n",
        "    # ---- Mixed pitch token ----\n",
        "    out = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(token):\n",
        "        ch = token[i]\n",
        "\n",
        "        if ch == \"-\":\n",
        "            # count consecutive dashes\n",
        "            dash_count = 0\n",
        "            while i < len(token) and token[i] == \"-\":\n",
        "                dash_count += 1\n",
        "                i += 1\n",
        "\n",
        "            while dash_count > 0:\n",
        "                if dash_count >= 4:\n",
        "                    out.append(\"<REST_4>\")\n",
        "                    dash_count -= 4\n",
        "                elif dash_count >= 3:\n",
        "                    out.append(\"<REST_3>\")\n",
        "                    dash_count -= 3\n",
        "                elif dash_count >= 2:\n",
        "                    out.append(\"<REST_2>\")\n",
        "                    dash_count -= 2\n",
        "                else:\n",
        "                    out.append(\"<REST_1>\")\n",
        "                    dash_count -= 1\n",
        "\n",
        "        elif ch in THAI_NOTES:\n",
        "            out.append(ch)\n",
        "            i += 1\n",
        "\n",
        "            # skip octave mark\n",
        "            if i < len(token) and token[i] in {UP_MARK, LOW_MARK}:\n",
        "                i += 1\n",
        "\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    return out if out else [\"<REST_1>\"]\n",
        "\n",
        "def song_to_pitch_sequence(song_json):\n",
        "    \"\"\"\n",
        "    Convert full song JSON into one flat pitch sequence.\n",
        "    Handles:\n",
        "    - normal string tokens\n",
        "    - dict blocks like {\"นำ\": [...]}, {\"ตาม\": [...]}\n",
        "    - nested lists\n",
        "    \"\"\"\n",
        "\n",
        "    sequence = []\n",
        "\n",
        "    def process_token(tok):\n",
        "        # Case 1: string token\n",
        "        if isinstance(tok, str):\n",
        "            sequence.extend(normalize_token(tok))\n",
        "\n",
        "        # Case 2: dict (นำ / ตาม block)\n",
        "        elif isinstance(tok, dict):\n",
        "            for key in tok:\n",
        "                for inner_tok in tok[key]:\n",
        "                    process_token(inner_tok)\n",
        "\n",
        "        # Case 3: list (nested structure)\n",
        "        elif isinstance(tok, list):\n",
        "            for inner_tok in tok:\n",
        "                process_token(inner_tok)\n",
        "\n",
        "    for section in song_json.get(\"sections\", []):\n",
        "        for bar in section.get(\"bars\", []):\n",
        "            for tok in bar:\n",
        "                process_token(tok)\n",
        "\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5J1bqLEbn2b"
      },
      "source": [
        "#### Apply to songs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ_afh1ReFob",
        "outputId": "34880ea8-f8f2-4e05-8b75-0b67ef83473e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1007\n",
            "['<REST_4>', '<REST_3>', 'ฟ', '<REST_1>', 'ฟ', 'ฟ', 'ฟ', '<REST_1>', 'ฟ', '<REST_1>', 'ฟ', '<REST_1>', 'ล', 'ด', 'ร', '<REST_1>', 'ฟ', '<REST_1>', 'ซ', 'ล', 'ซ', 'ด', 'ล', '<REST_1>', 'ซ', '<REST_1>', 'ฟ', '<REST_2>', 'ฟ', 'ซ', 'ล', '<REST_1>', 'ด', '<REST_1>', 'ร', '<REST_1>', 'ซ', 'ฟ', 'ร', '<REST_1>', 'ด', '<REST_1>', 'ล', '<REST_3>', 'ซ', '<REST_3>', 'ล', '<REST_2>', 'ร', 'ด', 'ล', 'ด', '<REST_1>', 'ร', '<REST_4>', '<REST_4>', '<REST_3>', 'ฟ', '<REST_3>', 'ซ', '<REST_2>', 'ฟ', 'ล', '<REST_1>', 'ซ', 'ฟ', 'ร', '<REST_4>', '<REST_4>', '<REST_3>', 'ฟ', '<REST_3>', 'ร', 'ด', 'ร', 'ฟ', 'ด', '<REST_4>', 'ซ', 'ล']\n"
          ]
        }
      ],
      "source": [
        "#Apply to songs\n",
        "for s in songs:\n",
        "    s[\"pitch_sequence\"] = song_to_pitch_sequence(s[\"data\"])\n",
        "\n",
        "print(len(songs[2][\"pitch_sequence\"]))\n",
        "print(songs[2][\"pitch_sequence\"][:80])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0HXLT07a0pU",
        "outputId": "a7c835e7-8283-4d29-ed4e-50245a11bdb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Note Count per Motif ===\n",
            "\n",
            "เขมร:\n",
            "  Total notes (including rests): 5936\n",
            "  Songs: 7\n",
            "  Avg notes per song: 848.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "note_count_by_motif = defaultdict(int)\n",
        "song_count_by_motif = defaultdict(int)\n",
        "\n",
        "for s in songs:\n",
        "    motif = s[\"motif\"]\n",
        "    note_count_by_motif[motif] += len(s[\"pitch_sequence\"])\n",
        "    song_count_by_motif[motif] += 1\n",
        "\n",
        "print(\"=== Note Count per Motif ===\\n\")\n",
        "\n",
        "for motif in sorted(note_count_by_motif.keys()):\n",
        "    total_notes = note_count_by_motif[motif]\n",
        "    total_songs = song_count_by_motif[motif]\n",
        "    avg_len = total_notes / total_songs\n",
        "\n",
        "    print(f\"{motif}:\")\n",
        "    print(f\"  Total notes (including rests): {total_notes}\")\n",
        "    print(f\"  Songs: {total_songs}\")\n",
        "    print(f\"  Avg notes per song: {avg_len:.2f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4udaD2JtT-zT"
      },
      "outputs": [],
      "source": [
        "# from thai_music_utils.eda_symbolic_normalization import flatten_song\n",
        "\n",
        "# pattern = \"ลซฟ\"\n",
        "# matches = []\n",
        "\n",
        "# for s in songs:\n",
        "#     if s[\"motif\"] != \"ลาว\":\n",
        "#         continue\n",
        "\n",
        "#     # flatten to normalized token list\n",
        "#     sequence = flatten_song(s[\"data\"])\n",
        "\n",
        "#     # merge into one continuous string\n",
        "#     seq_string = \"\".join(sequence)\n",
        "\n",
        "#     if pattern in seq_string:\n",
        "#         matches.append(s[\"song\"])\n",
        "\n",
        "# print(\"Songs under ลาว containing 'ลซฟ':\")\n",
        "# for name in matches:\n",
        "#     print(\"-\", name)\n",
        "\n",
        "# print(\"\\nTotal:\", len(matches))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KML2_rJWiZ7r"
      },
      "source": [
        "#### 2️⃣ Build Vocabulary (Token → Integer Mapping)\n",
        "\n",
        "Neural networks cannot process symbolic tokens directly.  \n",
        "We must convert each pitch token into a numeric ID.\n",
        "\n",
        "Steps:\n",
        "- Collect all unique tokens from all songs\n",
        "- Assign each token a unique integer\n",
        "- Build two mappings:\n",
        "  - `token_to_id`\n",
        "  - `id_to_token`\n",
        "\n",
        "This defines:\n",
        "- The model vocabulary\n",
        "- The input/output dimension for the LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WBnPXLuj3_g",
        "outputId": "3b020733-6bae-413a-b293-bf6481ca886c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['<REST_1>', '<REST_2>', '<REST_3>', '<REST_4>', 'ซ', 'ด', 'ท', 'ฟ', 'ม', 'ร', 'ล']\n",
            "Vocab size: 11\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Collect all tokens across songs\n",
        "all_tokens = []\n",
        "\n",
        "for s in songs:\n",
        "    all_tokens.extend(s[\"pitch_sequence\"])\n",
        "\n",
        "# Unique vocabulary\n",
        "vocab = sorted(set(all_tokens))\n",
        "\n",
        "# Create mappings\n",
        "token_to_id = {tok: i for i, tok in enumerate(vocab)}\n",
        "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MB3ENFPUkkEl"
      },
      "outputs": [],
      "source": [
        "# for s in songs:\n",
        "#     for tok in s[\"pitch_sequence\"]:\n",
        "#         if tok in [\"<\", \"R\", \"E\", \"S\", \"T\", \">\"]:\n",
        "#             print(\"BROKEN:\", s[\"song\"])\n",
        "#             break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEQQuDAeoHuq"
      },
      "source": [
        "#### 3️⃣ Convert Songs to Integer Sequences\n",
        "\n",
        "Neural networks cannot process symbolic tokens directly.  \n",
        "We must convert each pitch token into a numeric ID.\n",
        "\n",
        "Steps:\n",
        "- Take each song’s `pitch_sequence`\n",
        "- Replace each token with its corresponding integer ID using `token_to_id`\n",
        "- Store the new numeric sequence (e.g., `id_sequence`) inside each song\n",
        "\n",
        "This produces:\n",
        "- One integer sequence per song  \n",
        "- Shape per song: `(sequence_length,)`\n",
        "- Vocabulary size = `vocab_size`\n",
        "\n",
        "These integer sequences will be used to:\n",
        "- Create training samples (input → next token prediction)\n",
        "- Feed into the LSTM model\n",
        "- Compute loss over predicted next-token probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xfwNZTVqdcD",
        "outputId": "a2e87671-4766-4a36-d580-f0d5954e63a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example song length: 1007\n",
            "First 30 token IDs: [3, 2, 7, 0, 7, 7, 7, 0, 7, 0, 7, 0, 10, 5, 9, 0, 7, 0, 4, 10, 4, 5, 10, 0, 4, 0, 7, 1, 7, 4]\n"
          ]
        }
      ],
      "source": [
        "# 3️⃣ Convert Songs to Integer Sequences\n",
        "\n",
        "for s in songs:\n",
        "    s[\"id_sequence\"] = [\n",
        "        token_to_id[token]\n",
        "        for token in s[\"pitch_sequence\"]\n",
        "        if token in token_to_id\n",
        "    ]\n",
        "\n",
        "# sanity check\n",
        "print(\"Example song length:\", len(songs[2][\"id_sequence\"]))\n",
        "print(\"First 30 token IDs:\", songs[2][\"id_sequence\"][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djO6k8t_tRyH"
      },
      "source": [
        "#### 4️⃣ Prepare Training Sequences (LSTM Input Construction)\n",
        "\n",
        "The LSTM does not see full songs at once.\n",
        "\n",
        "Instead, we train it using sliding windows:\n",
        "\n",
        "Given a sequence:\n",
        "ด ร ม ซ ด ร ด ...\n",
        "\n",
        "We create training samples like:\n",
        "\n",
        "```\n",
        "Input (length = seq_len) → Target\n",
        "[ด ร ม ซ] → ด\n",
        "[ร ม ซ ด] → ร\n",
        "[ม ซ ด ร] → ด\n",
        "```\n",
        "\n",
        "This teaches the model:\n",
        "\"Given the previous N notes, predict the next note.\"\n",
        "\n",
        "Steps:\n",
        "- Choose a sequence length (e.g., 16)\n",
        "- Slide window across every song\n",
        "- Convert token IDs into (X, y) pairs\n",
        "- X shape: (num_samples, seq_len)\n",
        "- y shape: (num_samples,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hDrkBv2D-KEG"
      },
      "outputs": [],
      "source": [
        "for s in songs:\n",
        "    s[\"id_sequence\"] = [\n",
        "        token_to_id[token]\n",
        "        for token in s[\"pitch_sequence\"]\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hNXTElduAsh",
        "outputId": "fe5dd540-38e4-489d-ca8e-99e6c193bb8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (5824, 16)\n",
            "y shape: (5824,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "SEQ_LEN = 16  # number of previous notes to condition on\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for s in songs:\n",
        "    ids = s[\"id_sequence\"]\n",
        "\n",
        "    if len(ids) <= SEQ_LEN:\n",
        "        continue\n",
        "\n",
        "    for i in range(len(ids) - SEQ_LEN):\n",
        "        X.append(ids[i:i+SEQ_LEN])\n",
        "        y.append(ids[i+SEQ_LEN])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNROEXSAch8c"
      },
      "source": [
        "## 1️⃣ LSTM Model Definition\n",
        "\n",
        "We now define a baseline LSTM language model.\n",
        "\n",
        "Goal:\n",
        "- Input: sequence of 16 token IDs\n",
        "- Output: probability distribution over next token\n",
        "\n",
        "Architecture:\n",
        "- Embedding layer (token → dense vector)\n",
        "- LSTM layer\n",
        "- Linear output layer\n",
        "- Softmax handled by CrossEntropyLoss\n",
        "\n",
        "This is a standard neural language model setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRAJ9Og7XD7t",
        "outputId": "28b7125d-1c12-406b-af17-65b72cc36e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 0. PREREQUISITES\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbqUFxkyuuxz",
        "outputId": "cd9ca85e-f500-4b63-a93c-bd5bca056bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTMLanguageModel(\n",
            "  (embedding): Embedding(11, 64)\n",
            "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.25)\n",
            "  (fc): Linear(in_features=128, out_features=11, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ============================================================\n",
        "# LSTM Language Model (Improved Version)\n",
        "# - 2-layer stacked LSTM\n",
        "# - Dropout between layers\n",
        "# - Predict next token from last hidden state\n",
        "# ============================================================\n",
        "\n",
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, num_layers=2, dropout=0.25):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1️⃣ Embedding Layer\n",
        "        # Converts token IDs → dense vectors\n",
        "        # Shape: (batch, seq_len) → (batch, seq_len, embed_dim)\n",
        "        # --------------------------------------------------------\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        # 2️⃣ Stacked LSTM\n",
        "        # num_layers=2 → hierarchical pattern modeling\n",
        "        # dropout applied BETWEEN LSTM layers\n",
        "        # --------------------------------------------------------\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # 3️⃣ Final Linear Layer\n",
        "        # Maps final hidden state → vocabulary logits\n",
        "        # Shape: (batch, hidden_dim) → (batch, vocab_size)\n",
        "        # --------------------------------------------------------\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Convert tokens to embeddings\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Step 2: Pass through stacked LSTM\n",
        "        # output shape: (batch, seq_len, hidden_dim)\n",
        "        output, _ = self.lstm(x)\n",
        "\n",
        "        # Step 3: Take last timestep only\n",
        "        last_hidden = output[:, -1, :]\n",
        "\n",
        "        # Step 4: Project to vocab size\n",
        "        logits = self.fc(last_hidden)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Instantiate model\n",
        "# ------------------------------------------------------------\n",
        "model = LSTMLanguageModel(vocab_size).to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORmvqt93wwZU"
      },
      "source": [
        "### 1️⃣ Training Setup\n",
        "\n",
        "Now we train the LSTM as a next-token predictor.\n",
        "\n",
        "Task:\n",
        "Given 16 previous tokens → predict the next token.\n",
        "\n",
        "We define:\n",
        "\n",
        "• Loss: CrossEntropyLoss  \n",
        "  - Standard for multi-class classification  \n",
        "  - Compares predicted distribution vs true next-token  \n",
        "\n",
        "• Optimizer: Adam  \n",
        "  - Stable for sequence models  \n",
        "  - Good default for LSTM  \n",
        "\n",
        "• Training loop:\n",
        "  1. Forward pass\n",
        "  2. Compute loss\n",
        "  3. Backpropagation\n",
        "  4. Update weights\n",
        "  5. Repeat for multiple epochs\n",
        "\n",
        "Goal:\n",
        "Minimize next-token prediction loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cva2ty21wvn-"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.long).to(device)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
        "\n",
        "# Training config\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_samples = X_tensor.shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpWYLcG2xE6F",
        "outputId": "2fb82774-c3ab-4687-b038-b126b30d052f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/30: 100%|██████████| 91/91 [00:05<00:00, 16.89it/s, loss=0.635]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Average Loss: 0.5374\n",
            "Epoch 1 Perplexity: 1.7115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/30: 100%|██████████| 91/91 [00:07<00:00, 12.70it/s, loss=0.574]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Average Loss: 0.4845\n",
            "Epoch 2 Perplexity: 1.6233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/30: 100%|██████████| 91/91 [00:05<00:00, 17.12it/s, loss=0.578]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Average Loss: 0.4638\n",
            "Epoch 3 Perplexity: 1.5901\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/30: 100%|██████████| 91/91 [00:05<00:00, 15.59it/s, loss=0.521]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Average Loss: 0.4309\n",
            "Epoch 4 Perplexity: 1.5387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/30: 100%|██████████| 91/91 [00:06<00:00, 14.28it/s, loss=0.482]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Average Loss: 0.3998\n",
            "Epoch 5 Perplexity: 1.4915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/30: 100%|██████████| 91/91 [00:05<00:00, 17.14it/s, loss=0.488]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Average Loss: 0.3833\n",
            "Epoch 6 Perplexity: 1.4672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/30: 100%|██████████| 91/91 [00:07<00:00, 12.91it/s, loss=0.388]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Average Loss: 0.3488\n",
            "Epoch 7 Perplexity: 1.4174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/30: 100%|██████████| 91/91 [00:05<00:00, 18.02it/s, loss=0.401]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 Average Loss: 0.3263\n",
            "Epoch 8 Perplexity: 1.3859\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/30: 100%|██████████| 91/91 [00:05<00:00, 17.19it/s, loss=0.383]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 Average Loss: 0.3121\n",
            "Epoch 9 Perplexity: 1.3663\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/30: 100%|██████████| 91/91 [00:06<00:00, 13.01it/s, loss=0.375]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 Average Loss: 0.2934\n",
            "Epoch 10 Perplexity: 1.3410\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/30: 100%|██████████| 91/91 [00:05<00:00, 17.28it/s, loss=0.24]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 Average Loss: 0.2834\n",
            "Epoch 11 Perplexity: 1.3276\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/30: 100%|██████████| 91/91 [00:06<00:00, 13.75it/s, loss=0.279]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 Average Loss: 0.2658\n",
            "Epoch 12 Perplexity: 1.3045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/30: 100%|██████████| 91/91 [00:05<00:00, 16.14it/s, loss=0.186]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 Average Loss: 0.2423\n",
            "Epoch 13 Perplexity: 1.2742\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/30: 100%|██████████| 91/91 [00:05<00:00, 17.38it/s, loss=0.194]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 Average Loss: 0.2215\n",
            "Epoch 14 Perplexity: 1.2479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/30: 100%|██████████| 91/91 [00:07<00:00, 12.93it/s, loss=0.203]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 Average Loss: 0.2191\n",
            "Epoch 15 Perplexity: 1.2449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/30: 100%|██████████| 91/91 [00:05<00:00, 15.23it/s, loss=0.198]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 Average Loss: 0.2120\n",
            "Epoch 16 Perplexity: 1.2361\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/30: 100%|██████████| 91/91 [00:06<00:00, 13.17it/s, loss=0.228]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 Average Loss: 0.1973\n",
            "Epoch 17 Perplexity: 1.2181\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/30: 100%|██████████| 91/91 [00:05<00:00, 16.53it/s, loss=0.17]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 Average Loss: 0.1869\n",
            "Epoch 18 Perplexity: 1.2055\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/30: 100%|██████████| 91/91 [00:05<00:00, 17.31it/s, loss=0.186]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 Average Loss: 0.1765\n",
            "Epoch 19 Perplexity: 1.1931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/30: 100%|██████████| 91/91 [00:07<00:00, 12.87it/s, loss=0.171]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 Average Loss: 0.1690\n",
            "Epoch 20 Perplexity: 1.1841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/30: 100%|██████████| 91/91 [00:05<00:00, 17.16it/s, loss=0.0998]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 Average Loss: 0.1531\n",
            "Epoch 21 Perplexity: 1.1654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/30: 100%|██████████| 91/91 [00:06<00:00, 13.45it/s, loss=0.182]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 Average Loss: 0.1517\n",
            "Epoch 22 Perplexity: 1.1638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/30: 100%|██████████| 91/91 [00:05<00:00, 16.28it/s, loss=0.099]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 Average Loss: 0.1426\n",
            "Epoch 23 Perplexity: 1.1533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/30: 100%|██████████| 91/91 [00:05<00:00, 17.17it/s, loss=0.101]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 Average Loss: 0.1306\n",
            "Epoch 24 Perplexity: 1.1396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/30: 100%|██████████| 91/91 [00:07<00:00, 12.83it/s, loss=0.0746]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 Average Loss: 0.1335\n",
            "Epoch 25 Perplexity: 1.1428\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/30: 100%|██████████| 91/91 [00:05<00:00, 17.22it/s, loss=0.111]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 Average Loss: 0.1296\n",
            "Epoch 26 Perplexity: 1.1384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/30: 100%|██████████| 91/91 [00:06<00:00, 14.00it/s, loss=0.143]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27 Average Loss: 0.1267\n",
            "Epoch 27 Perplexity: 1.1351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/30: 100%|██████████| 91/91 [00:05<00:00, 15.75it/s, loss=0.117]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28 Average Loss: 0.1193\n",
            "Epoch 28 Perplexity: 1.1267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/30: 100%|██████████| 91/91 [00:05<00:00, 17.33it/s, loss=0.0982]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29 Average Loss: 0.1146\n",
            "Epoch 29 Perplexity: 1.1214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30/30: 100%|██████████| 91/91 [00:07<00:00, 12.92it/s, loss=0.0955]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30 Average Loss: 0.1097\n",
            "Epoch 30 Perplexity: 1.1159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import math  # put this once at top of notebook\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        range(0, num_samples, batch_size),\n",
        "        desc=f\"Epoch {epoch+1}/{epochs}\"\n",
        "    )\n",
        "\n",
        "    for i in progress_bar:\n",
        "\n",
        "        X_batch = X_tensor[i:i+batch_size]\n",
        "        y_batch = y_tensor[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / (num_samples // batch_size)\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    print(f\"Epoch {epoch+1} Perplexity: {perplexity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE7RjYXt27CM",
        "outputId": "dc3c0a08-41c2-4ea7-b164-99ef64220ece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights directory: /content/drive/MyDrive/thai_music_data/weights\n"
          ]
        }
      ],
      "source": [
        "WEIGHTS_DIR = DATA_ROOT / \"weights\"\n",
        "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Weights directory:\", WEIGHTS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E76O1nozb4V",
        "outputId": "e1571cec-b3a4-4b29-8c81-48e6d02ffe45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.save(model.state_dict(), WEIGHTS_DIR / \"lstm_pitch_only_khmer_35.pth\")\n",
        "print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBHsngM33lfX"
      },
      "source": [
        "## 2️⃣ Generation (Sampling)\n",
        "\n",
        "Now we use the trained LSTM to generate new symbolic pitch sequences.\n",
        "\n",
        "The model works as a next-token predictor:\n",
        "- Input: 16-token context window\n",
        "- Output: probability distribution over next token\n",
        "\n",
        "Generation procedure:\n",
        "1. Start with a seed sequence\n",
        "2. Predict next token\n",
        "3. Append prediction\n",
        "4. Slide window forward\n",
        "5. Repeat\n",
        "\n",
        "We use **temperature sampling**:\n",
        "- Low temperature (<1.0) → safer, repetitive\n",
        "- High temperature (>1.0) → more creative, unstable\n",
        "\n",
        "This lets us observe whether the model has learned meaningful Thai melodic structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RrS9dI83mbU",
        "outputId": "11623ff0-1cee-4b62-e8f5-f50dc5bec5cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMLanguageModel(\n",
              "  (embedding): Embedding(11, 64)\n",
              "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.25)\n",
              "  (fc): Linear(in_features=128, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "WEIGHTS_DIR = DATA_ROOT / \"weights\"\n",
        "\n",
        "model = LSTMLanguageModel(vocab_size).to(device)\n",
        "model.load_state_dict(\n",
        "    torch.load(\n",
        "        WEIGHTS_DIR / \"lstm_pitch_only_khmer_35.pth\",\n",
        "        map_location=device\n",
        "    )\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mqSdpBsA4P8z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_sequence(\n",
        "    model,\n",
        "    seed_ids,\n",
        "    max_new_tokens=100,\n",
        "    temperature=1.0,\n",
        "    seed=None   # ← add this\n",
        "):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    # 🔴 ADD THIS\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    generated = seed_ids.copy()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        context = generated[-16:]\n",
        "        x = torch.tensor(context, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "\n",
        "        logits = logits / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
        "        generated.append(next_id)\n",
        "\n",
        "    return generated\n",
        "\n",
        "def decode_ids(id_sequence):\n",
        "  return [id_to_token[i] for i in id_sequence]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jistvRx2aFfU"
      },
      "source": [
        "### 2.1 Generate from Existing Song (Paused)\n",
        "\n",
        "Seeds the model with the first `seq_len` tokens of a loaded song, then generates a continuation. Useful for side-by-side comparison with the original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "e0F8c06G55VM"
      },
      "outputs": [],
      "source": [
        "def generate_from_song(\n",
        "    song_idx,\n",
        "    model,\n",
        "    songs,\n",
        "    seq_len=16,\n",
        "    max_new_tokens=120,\n",
        "    temperature=0.8\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate continuation from a selected song\n",
        "    and print side-by-side comparison.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    song = songs[song_idx]\n",
        "    song_name = song[\"song\"]\n",
        "\n",
        "    print(f\"\\n=== Song Index: {song_idx} | Song: {song_name} ===\")\n",
        "\n",
        "    # ---- Seed ----\n",
        "    seed_ids = song[\"id_sequence\"][:seq_len]\n",
        "\n",
        "    # ---- Generate ----\n",
        "    generated_ids = generate_sequence(\n",
        "        model,\n",
        "        seed_ids=seed_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    # ---- Decode ----\n",
        "    original_ids = song[\"id_sequence\"][:seq_len + max_new_tokens]\n",
        "\n",
        "    original_tokens = decode_ids(original_ids)\n",
        "    generated_tokens = decode_ids(generated_ids)\n",
        "\n",
        "    # ---- Print ----\n",
        "    print(\"\\nSEED:\")\n",
        "    print(original_tokens[:seq_len])\n",
        "\n",
        "    print(\"\\nORIGINAL continuation:\")\n",
        "    print(original_tokens[seq_len:seq_len + 60])\n",
        "\n",
        "    print(\"\\nGENERATED continuation:\")\n",
        "    print(generated_tokens[seq_len:seq_len + 60])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gf0iqCh6J2u",
        "outputId": "e3cf7b2a-2535-4c07-cf0e-78bf5623fa15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Song Index: 22 | Song: แขกขาว ===\n",
            "\n",
            "SEED:\n",
            "['<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', 'ร', '<REST>', 'ร', 'ร', 'ร', '<REST>', 'ร', '<REST>', 'ร']\n",
            "\n",
            "ORIGINAL continuation:\n",
            "['<REST>', 'ม', '<REST>', 'ด', '<REST>', 'ร', '<REST>', 'ม', '<REST>', 'ฟ', '<REST>', 'ซ', '<REST>', '<REST>', '<REST>', 'ร', '<REST>', '<REST>', '<REST>', 'ซ', 'ล', 'ท', 'ด', 'ร', '<REST>', 'ร', 'ร', 'ร', '<REST>', 'ร', '<REST>', 'ร', '<REST>', 'ด', '<REST>', 'ด', '<REST>', '<REST>', '<REST>', '<REST>', 'ล', '<REST>', '<REST>', 'ด', 'ล', 'ซ', 'ฟ', '<REST>', 'ซ', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>']\n",
            "\n",
            "GENERATED continuation:\n",
            "['ด', 'ร', 'ฟ', 'ซ', 'ฟ', 'ร', 'ฟ', 'ด', 'ร', 'ร', '<REST>', 'ร', 'ร', 'ร', '<REST>', 'ร', '<REST>', 'ด', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', 'ล', '<REST>', 'ซ', 'ฟ', 'ซ', '<REST>', 'ล', '<REST>', '<REST>', 'ซ', 'ล', 'ด', 'ร', 'ฟ', 'ซ', 'ฟ', 'ร', 'ฟ', 'ด', 'ร', 'ซ', 'ล', 'ด', 'ร', 'ฟ', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', '<REST>', 'ร']\n"
          ]
        }
      ],
      "source": [
        "generate_from_song(\n",
        "    song_idx=22,\n",
        "    model=model,\n",
        "    songs=songs,\n",
        "    seq_len=16,\n",
        "    max_new_tokens=120,\n",
        "    temperature=1.2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uuwjxFOImK8"
      },
      "source": [
        "### 2.2 Generate from Raw Fragment\n",
        "\n",
        "Generate a continuation from a manually provided Thai notation fragment (slot-format strings like `\"---ฟ\"`).  \n",
        "The fragment is normalized, padded to `seq_len`, and fed as the seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOAlknXRJQCI",
        "outputId": "46670b81-27cf-4e34-d112-0612a3ae5e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 Longest Songs:\n",
            "\n",
            "1. เขมรโพธิสัตว์ | Motif: เขมร | Length: 1679\n",
            "2. เขมรพวง | Motif: เขมร | Length: 1007\n",
            "3. เขมรลออองค์ | Motif: เขมร | Length: 838\n",
            "4. เขมรชนบท | Motif: เขมร | Length: 759\n",
            "5. เขมรปากท่อ | Motif: เขมร | Length: 717\n"
          ]
        }
      ],
      "source": [
        "# Sort songs by length of pitch_sequence (descending)\n",
        "sorted_songs = sorted(\n",
        "    songs,\n",
        "    key=lambda s: len(s[\"pitch_sequence\"]),\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "print(\"Top 5 Longest Songs:\\n\")\n",
        "\n",
        "for i, s in enumerate(sorted_songs[:5]):\n",
        "    print(f\"{i+1}. {s['song']} | Motif: {s['motif']} | Length: {len(s['pitch_sequence'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWo8nZtNSxIu"
      },
      "source": [
        "### 2.3 Generation & Post-processing Helpers\n",
        "\n",
        "- `generate_from_fragment` — Normalizes a raw fragment, generates continuation, pretty-prints per bar  \n",
        "- `combine_fragment_and_generated` — Stitches original fragment (with octave marks) + generated continuation into a single slot list  \n",
        "- `slots_to_song_data` — Converts flat slot list back into song JSON format (8 slots per bar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "74nakIjKIk8D"
      },
      "outputs": [],
      "source": [
        "def generate_from_fragment(\n",
        "    fragment_tokens,\n",
        "    model,\n",
        "    token_to_id,\n",
        "    id_to_token,\n",
        "    seq_len=16,\n",
        "    max_new_tokens=120,\n",
        "    temperature=0.8,\n",
        "    bar_size=32,  # 1 bar = 32 slots\n",
        "    seed=None   # ← add this\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate continuation from manually provided JSON-style fragment.\n",
        "\n",
        "    Display:\n",
        "    - <REST> shown as \"-\"\n",
        "    - New line every bar (32 tokens)\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # ---- Normalize fragment ----\n",
        "    normalized = []\n",
        "    for tok in fragment_tokens:\n",
        "        normalized.extend(normalize_token(tok))\n",
        "\n",
        "    fragment_ids = [\n",
        "        token_to_id[t]\n",
        "        for t in normalized\n",
        "        if t in token_to_id\n",
        "    ]\n",
        "\n",
        "    if not fragment_ids:\n",
        "        print(\"⚠️ Fragment produced no valid tokens.\")\n",
        "        return None\n",
        "\n",
        "    # ---- Left pad ----\n",
        "    if len(fragment_ids) < seq_len:\n",
        "        pad_len = seq_len - len(fragment_ids)\n",
        "        fragment_ids = [token_to_id[\"<REST>\"]] * pad_len + fragment_ids\n",
        "    else:\n",
        "        fragment_ids = fragment_ids[-seq_len:]\n",
        "\n",
        "    # ---- Generate ----\n",
        "    generated_ids = generate_sequence(\n",
        "        model,\n",
        "        seed_ids=fragment_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    generated_tokens = [id_to_token[i] for i in generated_ids]\n",
        "\n",
        "\n",
        "    # ---- Convert REST_k back to proper dash count ----\n",
        "    pretty_stream = []\n",
        "\n",
        "    for t in generated_tokens[seq_len:]:\n",
        "        if t.startswith(\"<REST_\"):\n",
        "            k = int(t.replace(\"<REST_\", \"\").replace(\">\", \"\"))\n",
        "            pretty_stream.extend([\"-\"] * k)\n",
        "        else:\n",
        "            pretty_stream.append(t)\n",
        "\n",
        "    print(\"\\nGENERATED continuation:\\n\")\n",
        "\n",
        "    # ---- Print per bar ----\n",
        "    for i in range(0, len(pretty_stream), bar_size):\n",
        "        bar = pretty_stream[i:i+bar_size]\n",
        "\n",
        "        # group every 4 characters (1 slot)\n",
        "        grouped = [\n",
        "            \"\".join(bar[j:j+4])\n",
        "            for j in range(0, len(bar), 4)\n",
        "        ]\n",
        "\n",
        "        print(\", \".join(grouped))\n",
        "\n",
        "    return generated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LDzSX3Pfe6qc"
      },
      "outputs": [],
      "source": [
        "def combine_fragment_and_generated(\n",
        "    fragment_tokens,\n",
        "    generated_tokens,\n",
        "    normalize_token,\n",
        "    seq_len=16\n",
        "):\n",
        "    \"\"\"\n",
        "    Combine original fragment (preserving octave marks)\n",
        "    with generated continuation.\n",
        "\n",
        "    Returns dash-based slot list.\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1️⃣ Keep fragment EXACTLY as is\n",
        "    # -----------------------------\n",
        "    fragment_slots = fragment_tokens.copy()\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2️⃣ Remove seed duplication from generated\n",
        "    # -----------------------------\n",
        "    continuation = generated_tokens[seq_len:]\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3️⃣ Convert generated tokens back to dash string\n",
        "    # -----------------------------\n",
        "    generated_parts = []\n",
        "\n",
        "    for tok in continuation:\n",
        "        if tok.startswith(\"<REST_\"):\n",
        "            k = int(tok.replace(\"<REST_\", \"\").replace(\">\", \"\"))\n",
        "            generated_parts.append(\"-\" * k)\n",
        "        else:\n",
        "            generated_parts.append(tok)\n",
        "\n",
        "    generated_string = \"\".join(generated_parts)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4️⃣ Split generated part into 4-char slots\n",
        "    # -----------------------------\n",
        "    generated_slots = [\n",
        "        generated_string[i:i+4]\n",
        "        for i in range(0, len(generated_string), 4)\n",
        "    ]\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5️⃣ Combine fragment + generated\n",
        "    # -----------------------------\n",
        "    combined_slots = fragment_slots + generated_slots\n",
        "\n",
        "    return combined_slots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zEOnRMg7kTmf"
      },
      "outputs": [],
      "source": [
        "def slots_to_song_data(slots, title=\"Generated\"):\n",
        "\n",
        "    bars = [\n",
        "        slots[i:i+8]\n",
        "        for i in range(0, len(slots), 8)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"sections\": [\n",
        "            {\n",
        "                \"name\": \"Generated\",\n",
        "                \"bars\": bars\n",
        "            }\n",
        "        ]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AmXcJkmyj7Yv"
      },
      "outputs": [],
      "source": [
        "THAI_NOTES = \"ดรมฟซลท\"\n",
        "\n",
        "thai_base = {'ด': 58, 'ร': 60, 'ม': 62, 'ฟ': 63, 'ซ': 65, 'ล': 67, 'ท': 69}\n",
        "octave_offset = {1: -12, 2: 0, 3: 12}\n",
        "\n",
        "# Only these note–octave combos are allowed (16 total)\n",
        "allowed_oct = {\n",
        "    'ด': [2, 3],      # ด, ดํ\n",
        "    'ร': [2, 3],      # ร, รํ\n",
        "    'ม': [2, 3],      # ม, มํ\n",
        "    'ฟ': [2, 3],      # ฟ, ฟํ\n",
        "    'ซ': [1, 2, 3],   # ซฺ ซ ซํ\n",
        "    'ล': [1, 2, 3],   # ลฺ ล ลํ\n",
        "    'ท': [1, 2],      # ทฺ ท   (no ทํ)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPNlmE29S2El"
      },
      "source": [
        "## 3️⃣ Fragment Demo\n",
        "\n",
        "Generate a continuation from a hand-written Khmer-style fragment.  \n",
        "The fragment uses standard Thai slot notation (4 chars per slot, 8 slots per bar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWAVgAkbJdz7",
        "outputId": "65f3f7b7-3900-4749-b983-54f80bcf5ef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GENERATED continuation:\n",
            "\n",
            "-ด-ฟ, ซลดร, ดรฟซ, ลซฟร, ฟซ-ฟ, ---ด, รมฟซ, ---ล\n",
            "ซซซซ, ดรดด, ซลดร, ซฟฟฟ, ดรฟซ, -ลดซ, ลซฟร, ----\n",
            "---ล, ---ซ, ฟซลด, ---ร, ดมรร, ----, ----, ฟรดล\n",
            "-ซ-ฟ, --ลซ, ฟซ-ล, -ดฟร, ดรฟล, ซฟ--, ฟฟ-ฟ, --ลซ\n",
            "ฟร-ฟ, -ซ-ซ, -ซดร, ฟซฟล, ซฟลซ, ฟร--, มมซม, รดรด\n",
            "ลด-ร, -ฟลซ, ฟซ-ล, -ดฟร, ดรฟล, ซฟ--, ฟฟ-ฟ, --ลซ\n",
            "ฟร-ฟ, -ซลซ, ฟรฟด, -ร--, ----, -ร-ร, รร-ร, -รดร\n",
            "ฟซฟร, ฟดรด, ลด-ร, -ฟ--, --ฟซ, ลด-ร, -ฟ-ด, รฟ-ร\n",
            "---ด, ---ล, ----, ---ล, -ลลล, -ล-ล, ซล\n"
          ]
        }
      ],
      "source": [
        "fragment = [ \"----\", \"---ฟ\", \"-ฟฟฟ\", \"-ฟ-ฟ\", \"---ซ\", \"---ล\", \"---ดํ\", \"---รํ\",\n",
        "            \"-ฟํ-ดํ\", \"-รํ-ฟํ\", \"-ลํซํฟํ\", \"-ดํ-รํ\", \"ฟํรํดํล\", \"-ดํ--\", \"-ซ-ล\", \"ดํซลดํ\", \"----\" ]\n",
        "\n",
        "generated_tokens = generate_from_fragment(\n",
        "    fragment_tokens=fragment,\n",
        "    model=model,\n",
        "    token_to_id=token_to_id,\n",
        "    id_to_token=id_to_token,\n",
        "    seq_len=16,\n",
        "    max_new_tokens=240,\n",
        "    temperature=1.1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "\n",
        "# GENERATED continuation:\n",
        "\n",
        "# ---ด, -รฟด, รดลด, รฟ-ร, ---ล, ---ด, ---ฟ, ---ร\n",
        "# -ฟ-ซ, ลซฟร, ซฟรด, ฟรดล, ----, ---ล, -ลลล, -ล-ล\n",
        "# --ดร, ฟรดล, ดลซฟ, ดรฟซ, ลฟลซ, -ฟ-ร, ----, ----\n",
        "# -ม-ม, -ม-ม, ฟฟฟฟ, -ร-ร, ดดดด, -ล-ล, ซซซซ, -ล-ล\n",
        "# ดดดด, -ร-ร, -มฟร, ฟด-ร, -ฟ--, -ซ--, -ล--, -ด--\n",
        "# -ร-ฟ, ----, ---ฟ, -ฟฟฟ, -ฟ-ฟ, ลซฟร, ฟดรฟ, ดรฟซ\n",
        "# ลฟซล, -ม-ม, -ฟ-ร, -ด-ล, ซลดล, ซฟ-ซ, ----, ----\n",
        "# ฟซลซ, ฟร-ฟ, ----, ดรฟซ, -ฟ-ล, ซซ--, --ซล, ดรฟร\n",
        "# ฟซฟร, ฟด-ร, ----, ดรฟซ, ลซฟร, ดลดร, ดรฟซ, -ลดร\n",
        "# ดร"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtMtMLwvNhke"
      },
      "source": [
        "## 4️⃣ Post-processing & MIDI Export\n",
        "\n",
        "Pipeline: `combined_slots` → song JSON → DP octave inference → octave-mark-to-numeric conversion → Ranad MIDI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnfoeyiHf1_y",
        "outputId": "b0bc1ad4-579b-4d7a-c5df-50c256243a93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['----', '---ฟ', '-ฟฟฟ', '-ฟ-ฟ', '---ซ', '---ล', '---ดํ', '---รํ', '-ฟํ-ดํ', '-รํ-ฟํ', '-ลํซํฟํ', '-ดํ-รํ', 'ฟํรํดํล', '-ดํ--', '-ซ-ล', 'ดํซลดํ', '----', '-ด-ฟ', 'ซลดร', 'ดรฟซ', 'ลซฟร', 'ฟซ-ฟ', '---ด', 'รมฟซ', '---ล', 'ซซซซ', 'ดรดด', 'ซลดร', 'ซฟฟฟ', 'ดรฟซ', '-ลดซ', 'ลซฟร', '----', '---ล', '---ซ', 'ฟซลด', '---ร', 'ดมรร', '----', '----', 'ฟรดล', '-ซ-ฟ', '--ลซ', 'ฟซ-ล', '-ดฟร', 'ดรฟล', 'ซฟ--', 'ฟฟ-ฟ', '--ลซ', 'ฟร-ฟ', '-ซ-ซ', '-ซดร', 'ฟซฟล', 'ซฟลซ', 'ฟร--', 'มมซม', 'รดรด', 'ลด-ร', '-ฟลซ', 'ฟซ-ล', '-ดฟร', 'ดรฟล', 'ซฟ--', 'ฟฟ-ฟ', '--ลซ', 'ฟร-ฟ', '-ซลซ', 'ฟรฟด', '-ร--', '----', '-ร-ร', 'รร-ร', '-รดร', 'ฟซฟร', 'ฟดรด', 'ลด-ร', '-ฟ--', '--ฟซ', 'ลด-ร', '-ฟ-ด']\n"
          ]
        }
      ],
      "source": [
        "combined_slots = combine_fragment_and_generated(\n",
        "    fragment_tokens=fragment,\n",
        "    generated_tokens=generated_tokens,\n",
        "    normalize_token=normalize_token\n",
        ")\n",
        "\n",
        "print(combined_slots[:80])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts9hGpU4kXZu",
        "outputId": "d9abb40f-99fb-471d-8ee4-25d6f7a0f3f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'sections': [{'bars': [['----', '---ฟ', '-ฟฟฟ', '-ฟ-ฟ', '---ซ', '---ล', '---ดํ', '---รํ'],\n",
            "                        ['-ฟํ-ดํ', '-รํ-ฟํ', '-ลํซํฟํ', '-ดํ-รํ', 'ฟํรํดํล', '-ดํ--', '-ซ-ล', 'ดํซลดํ'],\n",
            "                        ['----', '-ด-ฟ', 'ซลดร', 'ดรฟซ', 'ลซฟร', 'ฟซ-ฟ', '---ด', 'รมฟซ'],\n",
            "                        ['---ล', 'ซซซซ', 'ดรดด', 'ซลดร', 'ซฟฟฟ', 'ดรฟซ', '-ลดซ', 'ลซฟร'],\n",
            "                        ['----', '---ล', '---ซ', 'ฟซลด', '---ร', 'ดมรร', '----', '----'],\n",
            "                        ['ฟรดล', '-ซ-ฟ', '--ลซ', 'ฟซ-ล', '-ดฟร', 'ดรฟล', 'ซฟ--', 'ฟฟ-ฟ'],\n",
            "                        ['--ลซ', 'ฟร-ฟ', '-ซ-ซ', '-ซดร', 'ฟซฟล', 'ซฟลซ', 'ฟร--', 'มมซม'],\n",
            "                        ['รดรด', 'ลด-ร', '-ฟลซ', 'ฟซ-ล', '-ดฟร', 'ดรฟล', 'ซฟ--', 'ฟฟ-ฟ'],\n",
            "                        ['--ลซ', 'ฟร-ฟ', '-ซลซ', 'ฟรฟด', '-ร--', '----', '-ร-ร', 'รร-ร'],\n",
            "                        ['-รดร', 'ฟซฟร', 'ฟดรด', 'ลด-ร', '-ฟ--', '--ฟซ', 'ลด-ร', '-ฟ-ด'],\n",
            "                        ['รฟ-ร', '---ด', '---ล', '----', '---ล', '-ลลล', '-ล-ล', 'ซล']],\n",
            "               'name': 'Generated'}],\n",
            " 'title': 'Generated'}\n"
          ]
        }
      ],
      "source": [
        "# Wrap combined slots into song JSON structure\n",
        "song_data_generated = slots_to_song_data(combined_slots)\n",
        "import pprint\n",
        "\n",
        "pprint.pprint(song_data_generated, width=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-I6kPcikaK0"
      },
      "outputs": [],
      "source": [
        "# Infer octaves using DP smoothness constraint (from thai_music_utils)\n",
        "song_data_auto = add_octaves_respecting_labels(song_data_generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMYbASYnknkH",
        "outputId": "d28df5af-4434-4dfe-bc14-8249b3439f0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'title': 'Generated',\n",
            " 'sections': [{'name': 'Generated',\n",
            "               'bars': [['----', '---ฟ', '-ฟฟฟ', '-ฟ-ฟ', '---ซ', '---ล', '---ดํ', '---รํ'],\n",
            "                        ['-ฟํ-ดํ', '-รํ-ฟํ', '-ลํซํฟํ', '-ดํ-รํ', 'ฟํรํดํล', '-ดํ--', '-ซ-ล', 'ดํซลดํ'],\n",
            "                        ['----', '-ดํ-ฟ', 'ซลดํรํ', 'ดํรํฟํซํ', 'ลํซํฟํรํ', 'ฟํซํ-ฟํ', '---ดํ', 'รํมํฟํซํ'],\n",
            "                        ['---ลํ', 'ซํซํซํซํ', 'ดํรํดํดํ', 'ซลดํรํ', 'ซฟฟฟ', 'ดรฟซ', '-ลดํซ', 'ลซฟร'],\n",
            "                        ['----', '---ล', '---ซ', 'ฟซลดํ', '---รํ', 'ดํมํรํรํ', '----', '----'],\n",
            "                        ['ฟํรํดํล', '-ซ-ฟ', '--ลซ', 'ฟซ-ล', '-ดํฟร', 'ดรฟล', 'ซฟ--', 'ฟฟ-ฟ'],\n",
            "                        ['--ลซ', 'ฟร-ฟ', '-ซ-ซ', '-ซดร', 'ฟซฟล', 'ซฟลซ', 'ฟร--', 'มมซม'],\n",
            "                        ['รดรด', 'ลฺด-ร', '-ฟลซ', 'ฟซ-ล', '-ดํฟร', 'ดรฟล', 'ซฟ--', 'ฟฟ-ฟ'],\n",
            "                        ['--ลซ', 'ฟร-ฟ', '-ซลซ', 'ฟรฟด', '-ร--', '----', '-ร-ร', 'รร-ร'],\n",
            "                        ['-รดร', 'ฟซฟร', 'ฟดรด', 'ลฺด-ร', '-ฟ--', '--ฟซ', 'ลดํ-รํ', '-ฟํ-ดํ'],\n",
            "                        ['รํฟํ-รํ', '---ดํ', '---ล', '----', '---ล', '-ลลล', '-ล-ล', 'ซล']]}]}\n"
          ]
        }
      ],
      "source": [
        "# Inspect: song data after octave inference\n",
        "from pprint import pprint\n",
        "\n",
        "pprint(song_data_auto, width=120, sort_dicts=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUt9PGlumnZO"
      },
      "outputs": [],
      "source": [
        "# Re-flatten octave-annotated song data back into slot list for MIDI export\n",
        "combined_slots = []\n",
        "\n",
        "for sec in song_data_auto[\"sections\"]:\n",
        "    for bar in sec[\"bars\"]:\n",
        "        if isinstance(bar, list):\n",
        "            combined_slots.extend(bar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9GELNqrlJmQ",
        "outputId": "d7d94235-2340-42d6-a854-df76a4bba264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ MIDI exported successfully!\n"
          ]
        }
      ],
      "source": [
        "sequence_string = \"\".join(combined_slots)\n",
        "\n",
        "# ============================================================\n",
        "# Convert Thai octave marks to numeric tags before MIDI export\n",
        "# ============================================================\n",
        "import re\n",
        "\n",
        "LOW_DOT = \"ฺ\"      # octave 1\n",
        "HIGH_DOT = \"ํ\"     # octave 3\n",
        "\n",
        "# Convert ฺ (LOW_DOT) → \"1\" after the note\n",
        "sequence_string = re.sub(rf\"([ดรมฟซลท]){LOW_DOT}\", r\"\\g<1>1\", sequence_string)\n",
        "\n",
        "# Convert ํ (HIGH_DOT) → \"3\" after the note\n",
        "sequence_string = re.sub(rf\"([ดรมฟซลท]){HIGH_DOT}\", r\"\\g<1>3\", sequence_string)\n",
        "\n",
        "# Notes without octave marks default to octave 2 in generate_ranad_midi\n",
        "\n",
        "midi_out = DATA_ROOT / \"generated.mid\" if not IS_COLAB else \"/content/generated.mid\"\n",
        "\n",
        "generate_ranad_midi(\n",
        "    sequence=sequence_string,\n",
        "    output_path=str(midi_out),\n",
        "    bpm=150,\n",
        "    global_transpose=12,\n",
        "    play_in_octave_pairs=True,\n",
        "    enable_roll=True\n",
        ")\n",
        "\n",
        "print(\"✅ MIDI exported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tew3GcDnNk1B"
      },
      "source": [
        "## 5️⃣ Evaluation\n",
        "\n",
        "Compare the generated output against a reference song from the corpus.  \n",
        "All metrics normalize `combined_slots` using the same `normalize_token` logic as training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DUW-Nz-Q02s"
      },
      "source": [
        "### 5.1 REST Type Distribution\n",
        "\n",
        "Compares the proportion of `<REST_1>` / `<REST_2>` / `<REST_3>` / `<REST_4>` between the original song and the generated output.  \n",
        "A well-trained model should produce a similar rhythmic density profile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7L_NYaCVQ0Z0"
      },
      "outputs": [],
      "source": [
        "def evaluate_rest_type_distribution_from_slots(combined_slots, song_name, songs, normalize_token):\n",
        "    \"\"\"\n",
        "    Compare REST_1/2/3/4 distribution between:\n",
        "    - Original song (pitch_sequence)\n",
        "    - Generated combined_slots (fragment + continuation)\n",
        "\n",
        "    Uses SAME normalize_token logic as training.\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1️⃣ Find original song\n",
        "    # -----------------------------\n",
        "    song_match = None\n",
        "    for s in songs:\n",
        "        if s[\"song\"] == song_name:\n",
        "            song_match = s\n",
        "            break\n",
        "\n",
        "    if not song_match:\n",
        "        print(f\"❌ Song '{song_name}' not found.\")\n",
        "        return\n",
        "\n",
        "    original_tokens = song_match[\"pitch_sequence\"]\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2️⃣ Normalize combined_slots\n",
        "    # -----------------------------\n",
        "    generated_tokens = []\n",
        "\n",
        "    for slot in combined_slots:\n",
        "        normalized = normalize_token(slot)\n",
        "        generated_tokens.extend(normalized)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3️⃣ Count REST types\n",
        "    # -----------------------------\n",
        "    rest_types = [\"<REST_1>\", \"<REST_2>\", \"<REST_3>\", \"<REST_4>\"]\n",
        "\n",
        "    def get_distribution(tokens):\n",
        "        total_rests = sum(1 for t in tokens if t.startswith(\"<REST\"))\n",
        "\n",
        "        counts = {r: 0 for r in rest_types}\n",
        "        for t in tokens:\n",
        "            if t in counts:\n",
        "                counts[t] += 1\n",
        "\n",
        "        proportions = {\n",
        "            r: (counts[r] / total_rests if total_rests > 0 else 0)\n",
        "            for r in rest_types\n",
        "        }\n",
        "\n",
        "        return counts, proportions, total_rests\n",
        "\n",
        "    orig_counts, orig_props, orig_total = get_distribution(original_tokens)\n",
        "    gen_counts, gen_props, gen_total = get_distribution(generated_tokens)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4️⃣ Print Results\n",
        "    # -----------------------------\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🎵 REST TYPE DISTRIBUTION — {song_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nOriginal Song:\")\n",
        "    print(f\"Total REST tokens: {orig_total}\")\n",
        "    for r in rest_types:\n",
        "        print(f\"{r}: {orig_counts[r]:4d} ({orig_props[r]*100:5.1f}%)\")\n",
        "\n",
        "    print(\"\\nGenerated (Fragment + Continuation):\")\n",
        "    print(f\"Total REST tokens: {gen_total}\")\n",
        "    for r in rest_types:\n",
        "        print(f\"{r}: {gen_counts[r]:4d} ({gen_props[r]*100:5.1f}%)\")\n",
        "\n",
        "    print(\"\\nAbsolute Proportion Differences:\")\n",
        "    for r in rest_types:\n",
        "        diff = abs(gen_props[r] - orig_props[r])\n",
        "        print(f\"{r}: {diff*100:5.1f}%\")\n",
        "\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"original\": orig_props,\n",
        "        \"generated\": gen_props\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tB544utQ_Cy",
        "outputId": "a625d24d-b051-44cd-b82c-934a2b032429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "🎵 REST TYPE DISTRIBUTION — เขมรพวง\n",
            "======================================================================\n",
            "\n",
            "Original Song:\n",
            "Total REST tokens: 277\n",
            "<REST_1>:  176 ( 63.5%)\n",
            "<REST_2>:   30 ( 10.8%)\n",
            "<REST_3>:   29 ( 10.5%)\n",
            "<REST_4>:   42 ( 15.2%)\n",
            "\n",
            "Generated (Fragment + Continuation):\n",
            "Total REST tokens: 77\n",
            "<REST_1>:   47 ( 61.0%)\n",
            "<REST_2>:   10 ( 13.0%)\n",
            "<REST_3>:   13 ( 16.9%)\n",
            "<REST_4>:    7 (  9.1%)\n",
            "\n",
            "Absolute Proportion Differences:\n",
            "<REST_1>:   2.5%\n",
            "<REST_2>:   2.2%\n",
            "<REST_3>:   6.4%\n",
            "<REST_4>:   6.1%\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'original': {'<REST_1>': 0.6353790613718412,\n",
              "  '<REST_2>': 0.10830324909747292,\n",
              "  '<REST_3>': 0.10469314079422383,\n",
              "  '<REST_4>': 0.15162454873646208},\n",
              " 'generated': {'<REST_1>': 0.6103896103896104,\n",
              "  '<REST_2>': 0.12987012987012986,\n",
              "  '<REST_3>': 0.16883116883116883,\n",
              "  '<REST_4>': 0.09090909090909091}}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_rest_type_distribution_from_slots(\n",
        "    combined_slots,\n",
        "    \"เขมรพวง\",\n",
        "    songs,\n",
        "    normalize_token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4Ev-v4ghM_i",
        "outputId": "0efc31ab-d090-4d61-e2dd-077d6895d5b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 200 normalized tokens:\n",
            "\n",
            "['<REST_4>', '<REST_3>', 'ฟ', '<REST_1>', 'ฟ', 'ฟ', 'ฟ', '<REST_1>', 'ฟ', '<REST_1>', 'ฟ', '<REST_3>', 'ซ', '<REST_3>', 'ล', '<REST_3>', 'ด', '<REST_3>', 'ร', '<REST_1>', 'ฟ', '<REST_1>', 'ด', '<REST_1>', 'ร', '<REST_1>', 'ฟ', '<REST_1>', 'ล', 'ซ', 'ฟ', '<REST_1>', 'ด', '<REST_1>', 'ร', 'ฟ', 'ร', 'ด', 'ล', '<REST_1>', 'ด', '<REST_2>', '<REST_1>', 'ซ', '<REST_1>', 'ล', 'ด', 'ซ', 'ล', 'ด', '<REST_4>', '<REST_3>', 'ด', '<REST_1>', 'ร', 'ฟ', 'ด', 'ร', 'ด', 'ล', 'ด', 'ร', 'ฟ', '<REST_1>', 'ร', '<REST_3>', 'ล', '<REST_3>', 'ด', '<REST_3>', 'ฟ', '<REST_3>', 'ร', '<REST_1>', 'ฟ', '<REST_1>', 'ซ', 'ล', 'ซ', 'ฟ', 'ร', 'ซ', 'ฟ', 'ร', 'ด', 'ฟ', 'ร', 'ด', 'ล', '<REST_4>', '<REST_3>', 'ล', '<REST_1>', 'ล', 'ล', 'ล', '<REST_1>', 'ล', '<REST_1>', 'ล', '<REST_2>', 'ด', 'ร', 'ฟ', 'ร', 'ด', 'ล', 'ด', 'ล', 'ซ', 'ฟ', 'ด', 'ร', 'ฟ', 'ซ', 'ล', 'ฟ', 'ล', 'ซ', '<REST_1>', 'ฟ', '<REST_1>', 'ร', '<REST_4>', '<REST_4>', '<REST_1>', 'ม', '<REST_1>', 'ม', '<REST_1>', 'ม', '<REST_1>', 'ม', 'ฟ', 'ฟ', 'ฟ', 'ฟ', '<REST_1>', 'ร', '<REST_1>', 'ร', 'ด', 'ด', 'ด', 'ด', '<REST_1>', 'ล', '<REST_1>', 'ล', 'ซ', 'ซ', 'ซ', 'ซ', '<REST_1>', 'ล', '<REST_1>', 'ล', 'ด', 'ด', 'ด', 'ด', '<REST_1>', 'ร', '<REST_1>', 'ร', '<REST_1>', 'ม', 'ฟ', 'ร', 'ฟ', 'ด', '<REST_1>', 'ร', '<REST_1>', 'ฟ', '<REST_2>', '<REST_1>', 'ซ', '<REST_2>', '<REST_1>', 'ล', '<REST_2>', '<REST_1>', 'ด', '<REST_2>', '<REST_1>', 'ร', '<REST_1>', 'ฟ', '<REST_4>', '<REST_3>', 'ฟ', '<REST_1>', 'ฟ', 'ฟ', 'ฟ', '<REST_1>', 'ฟ', '<REST_1>', 'ฟ']\n",
            "\n",
            "As flat sequence:\n",
            "\n",
            "<REST_4> <REST_3> ฟ <REST_1> ฟ ฟ ฟ <REST_1> ฟ <REST_1> ฟ <REST_3> ซ <REST_3> ล <REST_3> ด <REST_3> ร <REST_1> ฟ <REST_1> ด <REST_1> ร <REST_1> ฟ <REST_1> ล ซ ฟ <REST_1> ด <REST_1> ร ฟ ร ด ล <REST_1> ด <REST_2> <REST_1> ซ <REST_1> ล ด ซ ล ด <REST_4> <REST_3> ด <REST_1> ร ฟ ด ร ด ล ด ร ฟ <REST_1> ร <REST_3> ล <REST_3> ด <REST_3> ฟ <REST_3> ร <REST_1> ฟ <REST_1> ซ ล ซ ฟ ร ซ ฟ ร ด ฟ ร ด ล <REST_4> <REST_3> ล <REST_1> ล ล ล <REST_1> ล <REST_1> ล <REST_2> ด ร ฟ ร ด ล ด ล ซ ฟ ด ร ฟ ซ ล ฟ ล ซ <REST_1> ฟ <REST_1> ร <REST_4> <REST_4> <REST_1> ม <REST_1> ม <REST_1> ม <REST_1> ม ฟ ฟ ฟ ฟ <REST_1> ร <REST_1> ร ด ด ด ด <REST_1> ล <REST_1> ล ซ ซ ซ ซ <REST_1> ล <REST_1> ล ด ด ด ด <REST_1> ร <REST_1> ร <REST_1> ม ฟ ร ฟ ด <REST_1> ร <REST_1> ฟ <REST_2> <REST_1> ซ <REST_2> <REST_1> ล <REST_2> <REST_1> ด <REST_2> <REST_1> ร <REST_1> ฟ <REST_4> <REST_3> ฟ <REST_1> ฟ ฟ ฟ <REST_1> ฟ <REST_1> ฟ\n"
          ]
        }
      ],
      "source": [
        "# Convert combined_slots to normalized token stream\n",
        "normalized_combined = []\n",
        "\n",
        "for slot in combined_slots:\n",
        "    normalized_combined.extend(normalize_token(slot))\n",
        "\n",
        "# Print first 200 tokens for inspection\n",
        "print(\"First 200 normalized tokens:\\n\")\n",
        "print(normalized_combined[:200])\n",
        "\n",
        "# Optional: print as single string\n",
        "print(\"\\nAs flat sequence:\\n\")\n",
        "print(\" \".join(normalized_combined[:200]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyYCKKRAejvm"
      },
      "source": [
        "### 5.2 N-gram Overlap\n",
        "\n",
        "Measures what fraction of the generated n-grams (bigram / trigram / 4-gram) also appear in the reference song.  \n",
        "Higher overlap → the model is reproducing known melodic patterns.  \n",
        "Too high → possible memorization; too low → the output diverges from the style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5hsr4FYnelMQ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def evaluate_ngram_overlap(combined_slots, song_name, songs, normalize_token, n=3):\n",
        "    \"\"\"\n",
        "    Compute n-gram overlap between:\n",
        "    - Original song\n",
        "    - Generated (fragment + continuation)\n",
        "\n",
        "    n = 2 (bigram), 3 (trigram), 4 (quadgram)\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1️⃣ Find original song\n",
        "    # -----------------------------\n",
        "    song_match = None\n",
        "    for s in songs:\n",
        "        if s[\"song\"] == song_name:\n",
        "            song_match = s\n",
        "            break\n",
        "\n",
        "    if not song_match:\n",
        "        print(f\"❌ Song '{song_name}' not found.\")\n",
        "        return\n",
        "\n",
        "    original_tokens = song_match[\"pitch_sequence\"]\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2️⃣ Normalize combined_slots\n",
        "    # -----------------------------\n",
        "    generated_tokens = []\n",
        "    for slot in combined_slots:\n",
        "        generated_tokens.extend(normalize_token(slot))\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3️⃣ Build n-grams\n",
        "    # -----------------------------\n",
        "    def build_ngrams(tokens, n):\n",
        "        return [\n",
        "            tuple(tokens[i:i+n])\n",
        "            for i in range(len(tokens) - n + 1)\n",
        "        ]\n",
        "\n",
        "    orig_ngrams = set(build_ngrams(original_tokens, n))\n",
        "    gen_ngrams = build_ngrams(generated_tokens, n)\n",
        "\n",
        "    if not gen_ngrams:\n",
        "        print(\"⚠️ No generated n-grams.\")\n",
        "        return\n",
        "\n",
        "    overlap_count = sum(1 for g in gen_ngrams if g in orig_ngrams)\n",
        "\n",
        "    overlap_ratio = overlap_count / len(gen_ngrams)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4️⃣ Print results\n",
        "    # -----------------------------\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🎵 {n}-GRAM OVERLAP — {song_name}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Generated {n}-grams: {len(gen_ngrams)}\")\n",
        "    print(f\"Overlap count: {overlap_count}\")\n",
        "    print(f\"Overlap ratio: {overlap_ratio:.3f} ({overlap_ratio*100:.1f}%)\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return overlap_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRKrrU_5wj_a",
        "outputId": "86ec1f2c-6a5d-41ca-ccd2-91d0bee0086a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "🎵 2-GRAM OVERLAP — เขมรพวง\n",
            "======================================================================\n",
            "Generated 2-grams: 295\n",
            "Overlap count: 291\n",
            "Overlap ratio: 0.986 (98.6%)\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "🎵 3-GRAM OVERLAP — เขมรพวง\n",
            "======================================================================\n",
            "Generated 3-grams: 294\n",
            "Overlap count: 271\n",
            "Overlap ratio: 0.922 (92.2%)\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "🎵 4-GRAM OVERLAP — เขมรพวง\n",
            "======================================================================\n",
            "Generated 4-grams: 293\n",
            "Overlap count: 240\n",
            "Overlap ratio: 0.819 (81.9%)\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8191126279863481"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_ngram_overlap(combined_slots, \"เขมรพวง\", songs, normalize_token, n=2)\n",
        "evaluate_ngram_overlap(combined_slots, \"เขมรพวง\", songs, normalize_token, n=3)\n",
        "evaluate_ngram_overlap(combined_slots, \"เขมรพวง\", songs, normalize_token, n=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-deAWtvQLxaY"
      },
      "source": [
        "### 5.3 Pitch KL Divergence\n",
        "\n",
        "KL(P ‖ Q) where P = pitch distribution of the reference song and Q = pitch distribution of the generated output.  \n",
        "Measures how much the generated pitch usage diverges from the original.  \n",
        "Lower KL → closer match in overall pitch preference (e.g. how often ซ vs ด appears)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XUPjcRcbLytP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_pitch_kl(combined_slots, song_name, songs, normalize_token):\n",
        "    \"\"\"\n",
        "    Compute KL divergence between pitch distributions\n",
        "    of original song and generated output.\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------\n",
        "    # 1️⃣ Find original song\n",
        "    # -----------------------------\n",
        "    song_match = None\n",
        "    for s in songs:\n",
        "        if s[\"song\"] == song_name:\n",
        "            song_match = s\n",
        "            break\n",
        "\n",
        "    if not song_match:\n",
        "        print(f\"❌ Song '{song_name}' not found.\")\n",
        "        return\n",
        "\n",
        "    original_tokens = song_match[\"pitch_sequence\"]\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2️⃣ Normalize generated slots\n",
        "    # -----------------------------\n",
        "    generated_tokens = []\n",
        "    for slot in combined_slots:\n",
        "        generated_tokens.extend(normalize_token(slot))\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3️⃣ Extract pitch-only tokens\n",
        "    # -----------------------------\n",
        "    THAI_PITCHES = [\"ด\", \"ร\", \"ม\", \"ฟ\", \"ซ\", \"ล\", \"ท\"]\n",
        "\n",
        "    def get_pitch_distribution(tokens):\n",
        "        pitch_tokens = [t for t in tokens if t in THAI_PITCHES]\n",
        "        total = len(pitch_tokens)\n",
        "\n",
        "        counts = Counter(pitch_tokens)\n",
        "\n",
        "        probs = np.array([\n",
        "            counts[p] / total if total > 0 else 0\n",
        "            for p in THAI_PITCHES\n",
        "        ])\n",
        "\n",
        "        return probs\n",
        "\n",
        "    P = get_pitch_distribution(original_tokens)\n",
        "    Q = get_pitch_distribution(generated_tokens)\n",
        "\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    P = P + epsilon\n",
        "    Q = Q + epsilon\n",
        "\n",
        "    kl_div = np.sum(P * np.log(P / Q))\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4️⃣ Print Results\n",
        "    # -----------------------------\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"🎵 PITCH KL DIVERGENCE — {song_name}\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Original distribution:\", np.round(P, 3))\n",
        "    print(\"Generated distribution:\", np.round(Q, 3))\n",
        "    print(f\"\\nKL(P || Q): {kl_div:.4f}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return kl_div"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grzgmidiL3x5",
        "outputId": "73e4a6e3-fce3-408b-98d8-14b83aa0eb0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "🎵 PITCH KL DIVERGENCE — เขมรพวง\n",
            "======================================================================\n",
            "Original distribution: [0.237 0.173 0.019 0.253 0.162 0.152 0.004]\n",
            "Generated distribution: [0.196 0.187 0.033 0.263 0.148 0.172 0.   ]\n",
            "\n",
            "KL(P || Q): 0.0591\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.float64(0.05914725564600476)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_pitch_kl(combined_slots, \"เขมรพวง\", songs, normalize_token)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
